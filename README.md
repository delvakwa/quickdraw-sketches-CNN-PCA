# quickdraw-sketches-analysis
For my final project in the General Assembly Data Science Immersive course, I decided to fit a convolutional neural network (CNN) and principal component analysis (PCA) to Google's quickdraw dataset. This dataset is 11 gigs large and contains over 300 categories of drawings. There are 10,000 images in each category, and each drawing tells what country it was drawn in, as well as whether or not the computer was able to successfully rrecognize it as a category. I recently learned how to fit neural networks and saw some examples of CNN's being fit to image data, so I decided to give it a try on this dataset.

I began working on a Kaggle kernel becuse that's where I got my data. I found a few example Kaggle codes on how to pull, visualize, and model my data, and at first, everything was going well. The exploratory data analysis (EDA) wasn't too tedious and I was able to visualize my drawings and colorcode the strokes used to make each drawing. Unfortunately, things quickly turned sour when I was preprocessing in order to start fitting models. I quickly learned that a big problem with image data is figuring out how to store it effectively so that it's easy to manipulate with the two python libraries, Pandas and Keras. The code I was referencing to store my data suggested I save my images as an h5py file, which is a method of storing a large set of data and easily calling it with numpy. Unbeknowst to me, this method only worked for an older version of Kaggle and had been deprecated. To make matters worse, my access to the dataset changed, so I had to figure out a new way to call my data- which came with its own set of problems. To make a long story short, I completely changed my methodology for calling and visualizing my quickdraw dataset on Kaggle, and that was the extent of the work I did there.

I decided to move to a jupyter notebook and start working from there. Conveniently, there was a python package that contained the full quickdraw dataset, meaning that I should have simply been able to call that and get started on fitting the data to a CNN and running PCA on my data. That certainly would have been nice, but I ran into the same problem here: preprocessing image data for modeling is not simple. I had to save the drawings to disk, call them, and transform the pixel description matrices from RGB to black and white. Before transforming, there were 3 numbers (R, G, B) representing a single pixel, but after transforming there was only one. This saved up on memory and the time it would take to fit a CNN and perform PCA on the dataset. 

Once the preprocessing was complete, I realized that my memory limitations would prove to be a huge problem, so I decided to work with two categories of images, ants and anvils. I called 1,000 images for both and worked on storin the pixel information into a dataframe. Each drawings was 256 x 256 pixels, so storing 1,000 images of each category came up to 1.5 gigs each. I combined the two dataframes and fit a CNN to my data. The first batch of results took 45 minutes to fit and were severly overfit. I implemented early stopping, L2 regularization, and dropout to combat the error due to variance, and successfully generated a neural network on my data with 86% accuracy. I pickled those results and generated a plot representing what my results looked like at each of the CNN's epochs. 

My last task was to apply principal component analysis to my dataset. This required me to remove the column that stated whether or not a row of pixels belonged to an ant or an anvil, but once that was done I was able to see if there were any meaningful column combinations that attributed for the majority of the spread in the data. It's important to note that there are over 60,000 columns in this dataset, so the possibility of discovering that 3 or maybe even 1 column accounts for 70% of the spread of that data could be very interesting-- albeit unlikely. The first PCA row represents the columns that account for the largest spread of the data, second row accounts for the second largest spread, and so on. In order to truly understand the results given by PCA, I had to plot them because I am dealing with pixels. 

With the results of my PCA and CNN in hand, I have successfully generated a proof of concept. A CNN can be used to distinguish between different categories of data, and there are some pixels that are usually present when drawing an ant or an anvil. In the future, I would like to move this project to an AWS server so that I would worry less about storage and the time it took to fit models. I would also try to fit multiple neural networks to the data to see if I could get better accuracy results when classifying between drawing categories. It would be very interesting to compare category pairs (regardless of similarity) and see the PCA results from the pair as well as how accurate the neural network is at learning and deciphering between the two images. The last innovation I would implement is to perform PCA on the strokes it took to generate each image, and compare them to the PCA on the complete images themselves. This could reveal a lot of interesting relationships between drawings and the strokes used to generate them. Maybe a particular stroke is used in all images of angels. There is so much data and so much more that could be done. I'm very proud of the work and analysis I was able to do, and the posibilities of growth this project could take. 

I would like to thank my local instructor, Caroline Schmitt, and my global instructors Riley "Topo Chico" Dallas, Tim Book, and Matt Brems for teaching my about neural networks and data science in general. I could not have gotten this far without them. I would also like to thank Kaggle for helping me figure out a framework for completing this project, and last but not least, you for bearing through this README. I hope this project helps you one way or another, and I wish you the best on your coding projects. 
